{
  "questions": [
    "What reduction in size does DistilBERT achieve compared to BERT?",
    "How much of BERT's language understanding capabilities does DistilBERT retain?",
    "What is the speed increase of DistilBERT over BERT?",
    "Which datasets were used for training DistilBERT?",
    "What are the key components removed or altered in DistilBERT's architecture from BERT?",
    "What is the main technique used to train DistilBERT?",
    "How does DistilBERT's performance on the GLUE benchmark compare to BERT?",
    "How does DistilBERT handle the initialization of its student network from BERT?",
    "What are the main losses combined in DistilBERT's training objective?"
  ],
  "ground_truths": [
    "DistilBERT is 40% smaller than BERT.",
    "DistilBERT retains 97% of BERT's language understanding capabilities.",
    "DistilBERT is 60% faster than BERT.",
    "DistilBERT was trained on the same corpus as BERT: English Wikipedia and Toronto Book Corpus.",
    "DistilBERT removes token-type embeddings and pooler, and reduces the number of layers by a factor of 2.",
    "Knowledge distillation is the main technique used for training DistilBERT.",
    "DistilBERT retains 97% of BERT's performance on the GLUE benchmark with 40% fewer parameters.",
    "DistilBERT is initialized from BERT by taking one layer out of two for the student network.",
    "DistilBERT's training objective combines distillation loss with masked language modeling loss and cosine embedding loss."
  ]
}
